Links Fundamentales

Invitacion a la Competencia Kaggle : https://www.kaggle.com/t/a926346254804fa6be1d6a5ca04c3a11
dataset : https://storage.googleapis.com/dmeyf2022/competencia3_2022.csv.gz
El diccionario de datos es el mismo de la primer competencia : https://storage.googleapis.com/dmeyf2022/DiccionarioDatos.ods


para poder trabajar con el dataset desde Google Cloud, debe primero copiarlo en el bucket, tiene entre muchas, estas dos opciones:

Opcion 1 : usando el link del dataset, bajarlo primero a su PC y luego subirlo al bucket a la carpeta datasets desde https://console.cloud.google.com/storage/
Opcion 2 : abrir una maquina virtual y desde la consola de Ubuntu correr el comando :
wget https://storage.googleapis.com/dmeyf2022/competencia3_2022.csv.gz  -O   ~/buckets/b1/datasets/competencia3_2022.csv.gz
En caso de tener dudas sobre (ej machine size, si apagarla, si borrarla, RAM size, etc) sobre la opcion 2, elija de opcion 1



Objetivos Pedagógicos

Detectar catástrofes en los datos, y resolverlas ( campos "rotos" )
Enfrentarse al Data Drifting en todo su esplendor, y resolverlo seriamente
Aprender el arte de crear variables históricas
Entender el poder de entrenar en la union de varios periodos
Aprender a desarrollar una Training Strategy adecuada
en que porción del dataset hacer la optimización de los hiperparámetros
training
validation
testing
en que porción del dataset entrenar el modelo final
Utilizar la historia para generar workflows que generen modelos estables ( walk forward validation )
Utilizar undersampling de la clase mayoritaria para acelerar el procesamiento
Aprender a trabajar con "grandes" volúmenes de datos : millones de registros y miles de columnas



Reglas de la Segunda Competencia

El modelo final tiene que ser un solo modelo de { XGBoost, LightGBM, CatBoost} entrenado sobre todos los datos históricos que se desee, con todo el Feature Engineering histórico que se desee

Se podrá y deberá utilizar optimización de hiperparámetros, en particular optimización bayesiana.

Usted deberá correr su propia optimización bayesiana.
Se puede entrenar en la union de varios meses, o subconjuntos de lo anterior
Se deberá utilizar feature engineering histórico
Se deberán comparar modelos entrenados en un solo mes vs varios meses

No se pueden crear variables de feature engineering a partir de modelos

No se podrán utilizar ensemble de modelos.

La REPLICABILIDAD de todo su proceso es fundamental.